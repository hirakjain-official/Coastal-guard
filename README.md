# Social Media Analytics Agent

A comprehensive social media monitoring system designed to detect and analyze water-related hazards (floods, tsunamis, high waves, cyclones) across India through real-time social media analysis.

## ğŸŒŠ Overview

This system implements a **Phase 1 - Hourly Batch Processing** workflow that:

- **Fetches** social media posts every hour from Twitter/X using RapidAPI and official Twitter API
- **Processes** posts through deduplication, cleaning, and location inference  
- **Analyzes** content using DeepSeek AI for hazard classification and urgency assessment
- **Aggregates** results to detect geographical hotspots of hazard activity
- **Validates** findings through an analyst dashboard workflow
- **Alerts** relevant authorities and citizen applications when hazards are confirmed

### Key Features

âœ… **Multi-source Data Ingestion** - Twitter/X via RapidAPI and official API  
âœ… **Multi-language Support** - English + 11 Indian regional languages  
âœ… **AI-Powered Classification** - DeepSeek API for hazard type and urgency detection  
âœ… **Geospatial Hotspot Detection** - Cluster analysis for identifying affected areas  
âœ… **Human-in-the-Loop Validation** - Analyst dashboard for confirmation  
âœ… **Automated Alerting** - Integration with official dashboards and citizen apps  
âœ… **India-Focused** - Optimized for Indian geography, languages, and hazard patterns  

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PostgreSQL 12+ 
- Redis 6+
- API Keys for Twitter/X and DeepSeek

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd social-media-analytics-agent
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.template .env
   # Edit .env with your API keys and database credentials
   ```

5. **Initialize database** (optional - will use file storage by default)
   ```bash
   # Set up PostgreSQL and Redis if using database storage
   # Update DATABASE_URL and REDIS_URL in .env
   ```

6. **Run the application**
   ```bash
   # Test single batch
   python src/main.py --test
   
   # Start scheduled hourly processing
   python src/main.py
   ```

## ğŸ“Š Architecture Overview

### Phase 1 - Hourly Batch Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ• Scheduler   â”‚â”€â”€â”€â–¶â”‚  ğŸ“¥ Data Ingest  â”‚â”€â”€â”€â–¶â”‚ ğŸ§¹ Preprocessing â”‚
â”‚   (Every hour)  â”‚    â”‚ Twitter/X APIs   â”‚    â”‚ Clean & Dedupe  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸš¨ Alerts       â”‚â—€â”€â”€â”€â”‚ âœ… Validation    â”‚â—€â”€â”€â”€â”‚ ğŸ¤– AI Analysis  â”‚
â”‚ Official/Citizenâ”‚    â”‚ Analyst Review   â”‚    â”‚ DeepSeek API    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¾ Storage      â”‚â—€â”€â”€â”€â”‚ ğŸ“ Aggregation   â”‚â—€â”€â”€â”€â”‚                 â”‚
â”‚ Database/Files  â”‚    â”‚ Hotspot Detectionâ”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Scheduler** triggers every 60 minutes
2. **Data Ingestion** fetches posts from last hour using:
   - Twitter/X RapidAPI with India geo-filter + hazard keywords
   - Official Twitter API v2 with comprehensive search queries
3. **Preprocessing** performs:
   - Deduplication and retweet removal
   - Text cleaning and normalization  
   - Language detection (English + regional languages)
   - Location inference from geotags and text content
4. **AI Analysis** classifies each post:
   - Relevance: hazard vs non-hazard
   - Hazard Type: Flood, Tsunami, High Wave, Storm Surge, Cyclone, Other
   - Urgency: Low, Medium, High
   - Confidence Score: 0.0 to 1.0
5. **Aggregation** detects hotspots:
   - Groups posts by location + hazard type
   - Flags areas with â‰¥20 high-confidence posts in 10km radius
6. **Validation** sends to analyst dashboard:
   - Human review of detected hotspots
   - Confirm/reject decisions feed back into system
7. **Alerts** notify relevant channels:
   - Official emergency dashboards
   - Citizen mobile applications
   - Geographic push notifications

## ğŸ› ï¸ Configuration

### Environment Variables (.env)

```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/social_analytics
REDIS_URL=redis://localhost:6379/0

# API Keys
TWITTER_API_KEY=your_key
TWITTER_BEARER_TOKEN=your_token
RAPIDAPI_KEY=your_key
DEEPSEEK_API_KEY=your_key

# Application Settings
SCHEDULER_INTERVAL=3600  # 1 hour
CONFIDENCE_THRESHOLD=0.75
HOTSPOT_POST_THRESHOLD=20
HOTSPOT_RADIUS_KM=10

# Regional Settings  
SUPPORTED_LANGUAGES=en,hi,ta,te,bn,mr,gu,kn,ml,or,pa,as
TIMEZONE=Asia/Kolkata
```

### Regional Configuration (config/regional_config.yaml)

Contains India-specific:
- ğŸ—ºï¸ Geographic boundaries and coastal cities
- ğŸ—£ï¸ Multi-language hazard keywords  
- ğŸ“ Location aliases and variations
- ğŸŒ§ï¸ Seasonal risk factors
- âš¡ Priority alert regions

## ğŸ“ Usage Examples

### Running Single Test Batch
```bash
python src/main.py --test
```

### Starting Scheduled Processing  
```bash
python src/main.py
```

### Monitoring Logs
```bash
tail -f logs/social_analytics.log
```

### Checking Stored Results
```bash
# View processed batch files
ls data/processed/

# Examine specific batch
cat data/processed/batch_[id].json | jq .
```

## ğŸ§ª Development

### Project Structure
```
social-media-analytics-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Application entry point
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ scheduler.py        # Job scheduling  
â”‚       â”œâ”€â”€ data_ingestion.py   # Social media API clients
â”‚       â”œâ”€â”€ preprocessing.py    # Text cleaning & location inference
â”‚       â”œâ”€â”€ ai_analysis.py      # DeepSeek hazard classification
â”‚       â”œâ”€â”€ aggregation.py      # Hotspot detection
â”‚       â”œâ”€â”€ validation.py       # Analyst dashboard integration
â”‚       â”œâ”€â”€ alerts.py          # Notification systems
â”‚       â””â”€â”€ storage.py         # Database operations
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ regional_config.yaml   # India-specific configuration
â”‚   â””â”€â”€ database.yaml         # Database schema & settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Ingested posts
â”‚   â”œâ”€â”€ processed/            # Batch processing results
â”‚   â””â”€â”€ alerts/              # Generated alerts
â”œâ”€â”€ logs/                    # Application logs
â”œâ”€â”€ tests/                  # Unit tests
â””â”€â”€ scripts/               # Utility scripts
```

### Adding New Languages

1. Update `config/regional_config.yaml`:
   ```yaml
   hazard_keywords:
     new_language:
       flood: ["flood_word1", "flood_word2"]
       tsunami: ["tsunami_word1", "tsunami_word2"]
   ```

2. Update location aliases for city/state names in the new language

3. Test with sample posts in that language

### Adding New Hazard Types

1. Update AI analysis prompts in `src/modules/ai_analysis.py`
2. Add keywords to regional configuration
3. Update validation logic and alert templates
4. Test classification accuracy

### Testing

```bash
# Run unit tests
pytest tests/

# Test specific module
pytest tests/test_preprocessing.py

# Test with coverage
pytest --cov=src tests/
```

## ğŸ“ˆ Monitoring & Metrics

### Key Performance Indicators

- **Data Ingestion Rate**: Posts fetched per hour
- **Processing Latency**: Time from fetch to analysis completion  
- **Classification Accuracy**: AI model performance on hazard detection
- **Hotspot Detection Rate**: Geographic clusters identified per batch
- **Validation Response Time**: Analyst review turnaround
- **Alert Delivery Success**: Notification system reliability

### Log Analysis

```bash
# Monitor processing performance
grep "batch processing completed" logs/social_analytics.log

# Check API errors
grep "ERROR" logs/social_analytics.log | grep "API"

# View hotspot detections
grep "Detected.*hotspots" logs/social_analytics.log
```

## ğŸ”® Phase 2 - Future Enhancements

### Near Real-time Streaming
- Migrate from hourly batches to 15-minute or streaming processing
- Implement WebSocket connections for live data feeds
- Add event-driven architecture with message queues

### Enhanced AI Capabilities
- Fine-tune DeepSeek models on validated Indian hazard data
- Add image/video analysis for visual hazard detection
- Implement sentiment analysis for urgency assessment

### Advanced Analytics
- Predictive modeling for hazard forecasting
- Social network analysis for information spread tracking
- Integration with weather data and satellite imagery

### Expanded Coverage
- Support for additional social platforms (Facebook, Instagram, WhatsApp)
- Integration with news media and government sources
- Citizen reporting mobile application

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Contact

- **Issues**: Create a GitHub issue for bugs or feature requests
- **Documentation**: Check the `/docs` folder for detailed API documentation  
- **Community**: Join our Slack channel for discussions

---

**Built with â¤ï¸ for disaster preparedness and community safety in India** ğŸ‡®ğŸ‡³
